{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, StorageLevel\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import col, sum, concat_ws, count, desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster('local[*]').setAppName(\"CarsData\") \\\n",
    ".set(\"spark.driver.memory\", \"10g\") \\\n",
    ".set(\"spark.executor.memory\", \"12g\") \\\n",
    ".set(\"spark.driver.memoryOverhead\", \"512m\") \\\n",
    ".set(\"spark.executor.memoryOverhead\", \"512m\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.options(header=True, \n",
    "                        escape='\"', \n",
    "                        multiline=True,\n",
    "                        ).csv('hdfs://localhost:9000/cars/used_cars_data.csv')\n",
    "                        #).csv('hdfs://localhost:9000/user/mhhel/used_cars_data/used_cars_data.csv') #Mo's path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_count = data.count()\n",
    "column_count = len(data.columns)\n",
    "\n",
    "print(f\"Rows: {row_count}, Columns: {column_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based of our dataset we have decided that we want to discover and analyze the following:\n",
    "\n",
    "* Discover/look into what kind of make_name & model_name is the most popular car listed in a certain city.\n",
    "* Are certain types of cars more popular in the geographical sense of the US (say pick-up trucks in the southern part etc).\n",
    "* Which 5 cities have most car listings\n",
    "\n",
    "We decide to keep the following columns to work with based of what we think is relevant to our goals:\n",
    "\n",
    "- city: City where the car is listed\n",
    "- make_name: Make brand of the listed car\n",
    "- model_name: Model of the listed car\n",
    "- dealer_zip: Zipcode of the dealer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = [\"city\", \"make_name\", \"model_name\", \"dealer_zip\", \"engine_cylinders\"]\n",
    "\n",
    "cleaned_data = data.select(*columns_to_keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the new dataframe with it's columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.printSchema()\n",
    "cleaned_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "def plot_null_values(df):\n",
    "    \"\"\"\n",
    "    This function takes a PySpark DataFrame and plots the number of null values in each column.\n",
    "    \n",
    "    Args:\n",
    "    df (pyspark.sql.DataFrame): The PySpark DataFrame to analyze.\n",
    "    \n",
    "    Returns:\n",
    "    None: Displays a bar chart of null values per column.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Count the null values for each column\n",
    "    null_counts = df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
    "    \n",
    "    # Step 2: Collect the results into a dictionary where key is the column name, and value is the null count\n",
    "    null_counts_dict = null_counts.collect()[0].asDict()\n",
    "    \n",
    "    # Step 3: Prepare data for plotting\n",
    "    columns = list(null_counts_dict.keys())\n",
    "    null_values = [val / 3000040 for val in list(null_counts_dict.values())]\n",
    "    \n",
    "    # Step 4: Plot the data using Matplotlib\n",
    "    plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "    plt.barh(columns, null_values, color='skyblue',height=0.6)  # Create a horizontal bar plot\n",
    "    \n",
    "    plt.xlabel('Number of Null Values (scaled by 1/3000040)', fontsize=12)  # X-axis label\n",
    "    plt.ylabel('Columns', fontsize=12)  # Y-axis label\n",
    "    plt.title('Number of Null Values Per Column', fontsize=14)  # Title of the plot\n",
    "    \n",
    "    # Annotate the bar plot with the null counts\n",
    "    for i, val in enumerate(null_values):\n",
    "        plt.text(val + 0.01, i, f'{val:.4f}', ha='left', va='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()  # Ensure the layout fits well\n",
    "    plt.show()  # Display the plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_null_values(cleaned_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check for the the unique values of our features to see if there are any typos of any kind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.select('make_name').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.select('model_name').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.select('city').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.select('dealer_zip').distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FYI about the datatypes of the columns, we want to keep **dealer_zip** as a string because it preserves the leading zeros in the zip code, which good to have as the first number usually represents a broad region in the US."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most sold car in Houston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for records in Houston and create a \"Make + Model\" column\n",
    "houston_sales = cleaned_data.filter(col(\"city\") == \"Houston\")\n",
    "houston_sales = houston_sales.withColumn(\"Make_Model\", concat_ws(\" \", col(\"make_name\"), col(\"model_name\")))\n",
    "\n",
    "# Count the occurrences of each \"Make + Model\" and sort to find the most sold\n",
    "most_sold_houston = houston_sales.groupBy(\"Make_Model\").agg(count(\"*\").alias(\"count\")) \\\n",
    "                                  .orderBy(desc(\"count\")) \\\n",
    "                                  .limit(1)\n",
    "\n",
    "# Show the result\n",
    "most_sold_houston.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cities with most sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by city and count the number of occurrences\n",
    "city_sales_count = cleaned_data.groupBy(\"city\").agg(count(\"*\").alias(\"sales_count\"))\n",
    "\n",
    "# Order by the count in descending order and limit to top 5\n",
    "top_5_cities = city_sales_count.orderBy(desc(\"sales_count\")).limit(5)\n",
    "\n",
    "# Show the result\n",
    "top_5_cities.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most common engine type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by engine_cylinders and count the number of occurrences\n",
    "cylinder_count = cleaned_data.groupBy(\"engine_cylinders\").agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "# Order by the count in descending order and limit to top 5\n",
    "top_5_cylinders = cylinder_count.orderBy(desc(\"count\")).limit(5)\n",
    "\n",
    "# Show the result\n",
    "top_5_cylinders.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big_data_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
